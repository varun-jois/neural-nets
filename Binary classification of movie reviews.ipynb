{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification of movie reviews\n",
    "\n",
    "The objective of this notebook is to identify positive movie reviews. The movie reviews were obtained [rottentomatoes.com](https://www.rottentomatoes.com/) and the code that was used to do it can be found in my [scrapers repository](https://github.com/varun-jois/scrapers). In this analysis, 60 movies were chosen from various time periods from the genres of Action, Horror, Romance and Sports; 15 movies from each genre. The list of movies can be found in the file \"movies_list.xlsx\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the reviews and building the input for the neural network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reviews have already been obtained and can be found in the pickle \"movie_reviews.pickle\". First we shall load the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from NeuralNet import NeuralNet\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn import feature_extraction\n",
    "\n",
    "with open('movie_reviews.pickle', 'rb') as f:\n",
    "    movie_reviews = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of movie_reviews is a list of dicts where every list item corresponds to a movie.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keywords that will constitute our input units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we shall obtain the important words from our training set which will be used as our input units for our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get all the tokens\n",
    "all_tokens = []\n",
    "for m in movie_reviews:\n",
    "    for r in m['reviews']:\n",
    "        all_tokens.extend(word_tokenize(r['review']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converting all the tokens to lowercase and keeping only letters\n",
    "all_tokens = [t.lower() for t in all_tokens if t.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Obtaining the stopwords and removing it from all_tokens\n",
    "stop = set(stopwords.words('english'))\n",
    "all_tokens = [t for t in all_tokens if t not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating our lemmatizer to keep only the word stems (only getting the stems of nouns)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "all_tokens = [lemmatizer.lemmatize(t) for t in all_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27891"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating our bag-of-words\n",
    "bag = Counter(all_tokens)\n",
    "len(bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('movie', 16756), ('film', 13559), ('one', 7440), ('great', 5418), ('good', 4875), ('time', 4687), ('story', 4399), ('like', 3944), ('best', 3855), ('horror', 3563)]\n"
     ]
    }
   ],
   "source": [
    "# Let's see the 10 most common words\n",
    "print(bag.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our model, we shall take all the words that had at least 10 occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5500"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kw = [k for k, v in bag.items() if v >= 10]\n",
    "len(kw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are 5500 words that had at least 10 occurrences, our neural network will have 1820 input units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We convert our reviews to matrices "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On rottentomatoes, reviewers rate on a 5 point scale. We consider all reviews greater or equal to 4 as positive reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews = [r['review'] for m in movie_reviews for r in m['reviews']]\n",
    "ratings = [1 if r['rating'] >= 4 else 0 for m in movie_reviews for r in m['reviews']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vectorizing the data\n",
    "cv = feature_extraction.text.CountVectorizer(vocabulary=kw, binary=True)\n",
    "reviews_array = cv.fit_transform(reviews).toarray().T\n",
    "ratings_array = np.array(ratings).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shuffle the data to create training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# Shuffling the input units\n",
    "np.random.shuffle(reviews_array)\n",
    "\n",
    "# Shuffling the reviews and creating the tain and test data\n",
    "per = np.random.permutation(reviews_array.shape[1])\n",
    "reviews_array, ratings_array = reviews_array[:, per], ratings_array[:, per]\n",
    "train_index = round(reviews_array.shape[1] * 0.95)\n",
    "X_train, X_test = reviews_array[:, :train_index], reviews_array[:, train_index:]\n",
    "Y_train, Y_test = ratings_array[:, :train_index], ratings_array[:, train_index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check to see if the number of rows is the number of keywords (kw) and the number of columns is equal to the number of reviews in the training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our Neural Network\n",
    "We shall create our network using mini-batch gradient descent with the [ADAM](https://arxiv.org/pdf/1412.6980.pdf) optimizer. Our network will comprise of 2 hidden layers with 50 hidden units each. We will be using the tanh activation and hence we will be using the xavier intilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average cost for the 20th epoch was 0.1078769182205211.\n",
      "The average cost for the 40th epoch was 0.06163014755262198.\n",
      "The average cost for the 60th epoch was 0.04156796180617574.\n",
      "The average cost for the 80th epoch was 0.03156289006690934.\n",
      "The average cost for the 100th epoch was 0.02441794968370799.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "nn = NeuralNet()\n",
    "nn.initializer_xavier(X_train.shape[0], [50, 50, 1])\n",
    "nn.gd_mini_batch(X_train, Y_train, alpha=0.001, activation='tanh', \n",
    "                 mini_batch_size=256, epochs=100, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0082142194171153093"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the train set error\n",
    "predictions, error = nn.predict(X_train, Y_train)\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22839506172839508"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the test set error\n",
    "predictions, error = nn.predict(X_test, Y_test)\n",
    "error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
