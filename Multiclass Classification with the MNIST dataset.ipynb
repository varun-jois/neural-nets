{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Classification with the MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset contains images of handwritten digits ranging from 0-9, that have a size of 28x28. The training and test sets contain 60,000 and 10,000 samples respectively. It is considered a good dataset for experimenting with learning techniques because the data has already been processed and formatted and can be used as is. The datasets can be downloaded [here](http://yann.lecun.com/exdb/mnist/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from NeuralNet import NeuralNet, get_normalisation_constants\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from mnist import MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the dataset (as described [here](https://github.com/sorki/python-mnist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mndata = MNIST('.')\n",
    "images, labels = mndata.load_training()\n",
    "images_test, labels_test = mndata.load_testing()\n",
    "x_train = np.array(images).T\n",
    "y_train = np.array(labels)\n",
    "x_test = np.array(images_test).T\n",
    "y_test = np.array(labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a look at the labels we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our labels need to be binarized for our model to function, we use the LabelBinarizer function to transform our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "y_train = lb.fit_transform(y_train).T\n",
    "y_test = lb.fit_transform(y_test).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the dimensions of the data\n",
    "Since our neural network implementation takes the different examples in columns, our x_train should have a shape of (784, 60000). Our binarized y_train should have dimensions of (10, 60000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 60000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 60000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our neural network\n",
    "To improve the performance of our algorithm, we divide the data by the maximum value 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = x_train / 255\n",
    "x_test = x_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural network created with batch gradient descent\n",
    "We will first create a network using batch gradient descent. For this model, we shall use the tanh activation function and initialize our weights using the Xavier approach. Our network will have 2 hidden layers with 30 units each. We will train our model for 500 epochs and use a learning rate (alpha) of 1. We shall call this network nn_bgd. Please give the following code 5 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost after epoch 100 is 0.2677882150237599.\n",
      "The cost after epoch 200 is 0.2161813676077262.\n",
      "The cost after epoch 300 is 0.15435899294841698.\n",
      "The cost after epoch 400 is 0.1287763893882351.\n",
      "The cost after epoch 500 is 0.11097246470333756.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "nn_bgd = NeuralNet()\n",
    "nn_bgd.initializer_xavier(784, [30, 30, 10])\n",
    "nn_bgd.gd_batch(x_train, y_train, epochs=500, alpha=1, activation='tanh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost is constantly decreasing so we can be happy about the progress of learning. Now we get the predictions on the training set and calculate the training error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0307166666667\n"
     ]
    }
   ],
   "source": [
    "p, err = nn_bgd.predict(x_train, y_train)\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we calculate the test error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0393\n"
     ]
    }
   ],
   "source": [
    "p, err = nn_bgd.predict(x_test, y_test)\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had a training error of 3.07% and a test error of 3.93%. Since the difference is so low, we can rule out the possibility of overfitting and we need not consider regularization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural network created with mini-batch gradient descent\n",
    "We will now create a network using mini-batch gradient descent. In this case, we shall use the relu activation function and the [ADAM](https://arxiv.org/pdf/1412.6980.pdf) optimizer. For initiallizing the weights, we shall use the He method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average cost for the 20th epoch was 0.0555614290071635.\n",
      "The average cost for the 40th epoch was 0.02634966277642484.\n",
      "The average cost for the 60th epoch was 0.013635187938207866.\n",
      "The average cost for the 80th epoch was 0.010380860455697095.\n",
      "The average cost for the 100th epoch was 0.005580254005634052.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "nn_mbgd = NeuralNet()\n",
    "nn_mbgd.initializer_he(784, [30, 30, 10])\n",
    "nn_mbgd.gd_mini_batch(x_train, y_train, epochs=100, alpha=.001, activation='relu', mini_batch_size=256, optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate training error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0102833333333\n"
     ]
    }
   ],
   "source": [
    "p, err = nn_mbgd.predict(x_train, y_train)\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we calculate the test error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.041\n"
     ]
    }
   ],
   "source": [
    "p, err = nn_mbgd.predict(x_test, y_test)\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With mini-batch gradient descent we got equally impressive error rates of 1.02% and 4.1% for the training and test data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
